import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sb
from matplotlib.colors import ColorConverter
from skimage import data
from skimage.color import rgb2gray
from sklearn.preprocessing import OneHotEncoder
from matplotlib.colors import ColorConverter
from skimage import data
from skimage.color import rgb2gray
from sklearn.model_selection import train_test_split
import keras
from keras.models import Sequential
from keras.layers import Dense,MaxPool2D,Conv2D,Flatten
from keras import optimizers
from keras.layers import Dropout
import tensorflow as tf
import pandas as pd
from sklearn.preprocessing import OneHotEncoder
import numpy as np
import matplotlib.pyplot as plt

url = 'https://raw.githubusercontent.com/spencerbrady17/CNNforBG/main/DATA_CNN.csv'
df=pd.read_csv(url,encoding= 'unicode_escape')
df=df.dropna(how="all")
df=df.iloc[:,:3]
df_columns = {
    "REFCODE": str, 
    "HOMO-LUMO Gap" : np.float32,
    "SMILES Code": str
}
df.columns = df_columns.keys()
df = df.astype(df_columns)
df.head(5)
def encoding(df):
  encoded=[]
  disposed_indices = []
  elements=[['c'], ['n'], ['o'], ['C'], ['N'],  ['O'],
            ['F'], ['Cl'], ['S'],['Br'],
            ['('], [')'], ['='], ['#'],['/'],['\\']]
  elements+=[[str(x)] for x in range(1,10)]
  enc = OneHotEncoder(handle_unknown='ignore')
  enc.fit(elements)
  enc.categories_
  df1=pd.Series(df["SMILES Code"])
  for i in range(df1.shape[0]):
    if(len(df1[i]) > 100): 
      disposed_indices.append(i)
      continue                         # Limit number of SMILES chars to 100 (only missing 10 out of thousands is negligible)
    x=enc.transform([[x] for x  in df1[i]]).toarray()
    y=np.zeros(((100-x.shape[0]),len(elements)))            # To make this more automatic, the number 100 would instead be the maximum number of chars in a SMILES code
    encoded.append(np.vstack((x,y)))
  return (encoded, disposed_indices)

def encoded_generate_images(df):
  (listt,temp)=encoding(df)
  plt.figure(figsize=(20,100))
  for i in range(len(listt)):
    plt.subplot(len(listt),5,i+1)
    plt.imshow(listt[i])
    
encoded_generate_images(df.head(10))
df.head(5)

def encoding(df):
  encoded=[]
  disposed_indices = []
  elements=[['c'], ['n'], ['o'], ['C'], ['N'],  ['O'],
            ['F'], ['Cl'], ['S'],['Br'],
            ['('], [')'], ['='], ['#'],['/'],['\\']]
  elements+=[[str(x)] for x in range(1,10)]
  enc = OneHotEncoder(handle_unknown='ignore')
  enc.fit(elements)
  enc.categories_
  df1=pd.Series(df["SMILES Code"])
  for i in range(df1.shape[0]):
    if(len(df1[i]) > 100): 
      disposed_indices.append(i)
      continue                         # Limit number of SMILES chars to 100 (only missing 10 out of thousands is negligible)
    x=enc.transform([[x] for x  in df1[i]]).toarray()
    y=np.zeros(((100-x.shape[0]),len(elements)))            # To make this more automatic, the number 100 would instead be the maximum number of chars in a SMILES code
    encoded.append(np.vstack((x,y)))
  return (encoded, disposed_indices)

def encoded_generate_images(df):
  (listt,temp)=encoding(df)
  plt.figure(figsize=(20,100))
  for i in range(len(listt)):
    plt.subplot(len(listt),5,i+1)
    plt.imshow(listt[i])
    
X,disposed_indices=encoding(df)
removed_mols = df.iloc[disposed_indices]
Y=df["HOMO-LUMO Gap"].drop(labels=disposed_indices, axis=0)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.10, random_state=0)

X_train = np.array(X_train)
X_test = np.array(X_test)
y_train = np.array(y_train)
y_test = np.array(y_test)
X_test = X_test.reshape(len(X_test), 100, 25, 1)
X_train = X_train.reshape(len(X_train), 100, 25, 1)
y_train = np.array(y_train)
y_test = np.array(y_test)
y_train = y_train.reshape(len(y_train), 1)
y_test = y_test.reshape(len(y_test), 1)

model=Sequential()
model.add(Conv2D(64,kernel_size=3, input_shape=(100,25,1), activation="relu"))
model.add(Conv2D(32,kernel_size=3, activation="relu"))
model.add(Flatten())
model.add(Dense(32,activation="relu",kernel_regularizer="l2"))
model.add(Dropout(0.1))
model.add(Dense(10))
model.add(Dense(1, activation="relu"))
#opt=keray_test[:10]s.adam(learning_rate=0.03)
model.compile(optimizer='adam',loss="mean_absolute_error",)
Model_rms_1=model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=100)

print (model.predict(X_test[:10]))
y_predtest = model.predict(X_test)
y_predtrain = model.predict(X_train)

print (y_test[:10])
experimental = y_test

percent_diff = abs((y_predtest - experimental)/((y_predtest + experimental)/2))*100
average = sum(percent_diff)/len(percent_diff)
print(average)


import plotly.graph_objs as go
fig = go.Figure()

fig.add_trace(go.Scatter(
    x=y_predtest.reshape(y_test.shape), y=y_test.reshape(y_test.shape),
    name='sin',
    mode='markers',
    marker_color='rgba(100, 10, 90, .8)'))

fig.update_traces(mode='markers', marker_line_width=2, marker_size=13)
fig.update_layout(
    
    yaxis_title="Exprimental Homo-Lumo Gap",
    xaxis_title="Predicted Homo-Lumo Gap",
    title="Test Set",
    
    font=dict(
        family="arial",
        size=24,
        color="black"
    )
    )
                  
fig.show()

plt.figure(figsize=(15,7))
plt.plot(Model_rms_1.history["loss"],color='r', linewidth=2.75, linestyle='-', marker='.', markersize=15,markerfacecolor='b',
         markeredgecolor='black',
         markeredgewidth=1,label="Training Loss")
plt.plot(Model_rms_1.history['val_loss'],color='black', linewidth=2.75, linestyle='-', marker='.', markersize=15,markerfacecolor='brown',
         markeredgecolor='black',
         markeredgewidth=1,label="Validation Loss")
plt.xlabel("Number of Epochs",size=20,color="black")
plt.ylabel("Loss",size=20,color="black")
plt.xticks(size=20)
plt.yticks(size=20)
plt.legend()


plt.show()
